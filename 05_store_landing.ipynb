{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97414eea-e6e9-44db-9526-a9586167ef80",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# script to create store dimension landing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51418b25-b8fb-4856-88b2-778765c992d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "from lib.spark_session import get_spark_session\n",
    "from lib.utils import date_data, get_string_cols, get_rundate\n",
    "from lib.job_control import insert_log, get_max_timestamp\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.functions import current_timestamp, lit\n",
    "from datetime import datetime\n",
    "from delta import DeltaTable\n",
    "from lib.aws_s3 import archive_landing_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0bbadd45-9f61-4270-8943-4995947f0dc0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPARK_APP: JOB triggered for rundate - 20220101\n"
     ]
    }
   ],
   "source": [
    "# JOB Parameters\n",
    "rundate = get_rundate()\n",
    "schema_name = \"edw_ld\"\n",
    "table_name = \"dim_store_ld\"\n",
    "table_full_name = f\"{schema_name}.{table_name}\" \n",
    "landing_file_name = f\"store_{rundate}.csv\"\n",
    "landing_file_path = f\"s3a://datasatish/dw-with-pyspark/landing/store/{landing_file_name}\"\n",
    "print(\"SPARK_APP: JOB triggered for rundate - \" + rundate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d48cc1c9-8578-4e91-ac0d-a9595ba6ace9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPARK_APP: Spark UI - http://16804892cba9:4040\n"
     ]
    }
   ],
   "source": [
    "# Generate Spark Session\n",
    "spark: SparkSession = get_spark_session(f\"Landing load - {table_full_name}\")\n",
    "print(\"SPARK_APP: Spark UI - \" + spark.sparkContext.uiWebUrl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f326a9f-b69e-4c17-adce-e62ac5d23edd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Spark Configs\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 8)\n",
    "spark.conf.set(\"spark.sql.parquet.mergeSchema\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33f36549-0196-49dc-84c1-4ffbf40e3c6a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPARK_APP: Printing Raw Schema --\n",
      "root\n",
      " |-- store_id: string (nullable = true)\n",
      " |-- store_name: string (nullable = true)\n",
      " |-- address: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- zip_code: string (nullable = true)\n",
      " |-- phone_number: string (nullable = true)\n",
      "\n",
      "SPARK_APP: Landing data count - 7\n"
     ]
    }
   ],
   "source": [
    "# Create Raw dataframe and get col info\n",
    "df_raw = spark \\\n",
    "    .read \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"header\", True) \\\n",
    "    .load(landing_file_path)\n",
    "\n",
    "print(\"SPARK_APP: Printing Raw Schema --\")\n",
    "df_raw.printSchema()\n",
    "\n",
    "# Get Landing count\n",
    "print(\"SPARK_APP: Landing data count - \" + str(df_raw.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3860edd3-abdf-42d6-90cc-cd34a675c17a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPARK_APP: Casted all columns to String\n",
      "SPARK_APP: Added AUDIT column\n",
      "SPARK_APP: Final layer data count - 7\n",
      "SPARK_APP: Printing Landing Schema --\n",
      "root\n",
      " |-- store_id: string (nullable = true)\n",
      " |-- store_name: string (nullable = true)\n",
      " |-- address: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- zip_code: string (nullable = true)\n",
      " |-- phone_number: string (nullable = true)\n",
      " |-- insert_dt: timestamp (nullable = false)\n",
      " |-- rundate: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cast all columns to String\n",
    "df_casted = df_raw.selectExpr(get_string_cols(spark, df_raw))\n",
    "print(\"SPARK_APP: Casted all columns to String\")\n",
    "\n",
    "# Add audit columns\n",
    "df_ld = df_casted.withColumn(\"insert_dt\", current_timestamp()) \\\n",
    "    .withColumn(\"rundate\", lit(rundate))\n",
    "print(\"SPARK_APP: Added AUDIT column\")\n",
    "\n",
    "# Get Final Layer count\n",
    "print(\"SPARK_APP: Final layer data count - \" + str(df_ld.count()))\n",
    "print(\"SPARK_APP: Printing Landing Schema --\")\n",
    "df_ld.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3f67b7c-0a34-431c-9601-2194fc095812",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPARK_APP: Data written to landing layer\n"
     ]
    }
   ],
   "source": [
    "# Write the data to landing layer checking if table exists\n",
    "if get_max_timestamp(spark, schema_name, table_name) != \"1900-01-01 00:00:00.000000\":\n",
    "    df_ld.write \\\n",
    "        .format(\"delta\") \\\n",
    "        .mode(\"append\") \\\n",
    "        .saveAsTable(table_full_name)\n",
    "else:\n",
    "    df_ld.write \\\n",
    "        .format(\"delta\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .saveAsTable(table_full_name)\n",
    "\n",
    "print(\"SPARK_APP: Data written to landing layer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "395a183d-6987-4400-9b38-d5cc6025a945",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/boto3/compat.py:82: PythonDeprecationWarning: Boto3 will no longer support Python 3.7 starting December 13, 2023. To continue receiving service updates, bug fixes, and security updates please upgrade to Python 3.8 or later. More information can be found here: https://aws.amazon.com/blogs/developer/python-support-policy-updates-for-aws-sdks-and-tools/\n",
      "  warnings.warn(warning, PythonDeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to locate credentials\n",
      "SPARK_APP: ERROR - Landing file store_20220101.csv not archived. Please Archive the file manually\n"
     ]
    }
   ],
   "source": [
    "# Archive the landing file\n",
    "# (UPDATE: Python version in docker as 3.7 is not supported.)\n",
    "if archive_landing_object(landing_file_name, \"store\"):\n",
    "    print(f\"SPARK_APP: Landing file {landing_file_name} archived\")\n",
    "else:\n",
    "    print(f\"SPARK_APP: ERROR - Landing file {landing_file_name} not archived. Please Archive the file manually\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "764632d8-5614-436f-860c-9b4accba4164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPARK_APP: Update JOB Control Log\n"
     ]
    }
   ],
   "source": [
    "# Add job details in JOB CONTROL\n",
    "insert_log(spark, schema_name, table_name, datetime.now(), rundate)\n",
    "print(\"SPARK_APP: Update JOB Control Log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "05fa526a-61c5-4661-aa28-c7923c30124d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+--------------------------+--------+-------------------------+\n",
      "|schema_name|table_name  |max_timestamp             |rundate |insert_dt                |\n",
      "+-----------+------------+--------------------------+--------+-------------------------+\n",
      "|edw_ld     |dim_store_ld|2025-12-28 17:56:12.375593|20220101|2025-12-28 17:56:17.96417|\n",
      "+-----------+------------+--------------------------+--------+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"select * from edw.job_control where table_name = '{table_name}' order by insert_dt desc limit 1\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7b88bf0a-ea78-4795-996f-3d27f40482ee",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------------+---------------------+--------------------+-------------+\n",
      "|version|executionTimeMs|numTargetRowsInserted|numTargetRowsUpdated|numOutputRows|\n",
      "+-------+---------------+---------------------+--------------------+-------------+\n",
      "|2      |null           |null                 |null                |7            |\n",
      "+-------+---------------+---------------------+--------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the logs from delta table version\n",
    "dt = DeltaTable.forName(spark, table_full_name)\n",
    "dt.history().limit(1).select(\"version\",\"operationMetrics.executionTimeMs\", \n",
    "                                 \"operationMetrics.numTargetRowsInserted\",\n",
    "                                \"operationMetrics.numTargetRowsUpdated\",\n",
    "                                \"operationMetrics.numOutputRows\").show(1, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3e76708d-4f06-470e-8135-2a56d08fdafd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPARK_APP: Symlink Manifest file generated\n"
     ]
    }
   ],
   "source": [
    "# Generate Symlink manifest for Athena Access\n",
    "dt.generate(\"symlink_format_manifest\")\n",
    "print(\"SPARK_APP: Symlink Manifest file generated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "925a9c07-47c8-43a2-a110-d2664f37d8a1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+--------------+-----------+-----+--------+--------------+--------------------+--------+\n",
      "|store_id|  store_name|       address|       city|state|zip_code|  phone_number|           insert_dt| rundate|\n",
      "+--------+------------+--------------+-----------+-----+--------+--------------+--------------------+--------+\n",
      "|    S001|Pet House KA|   123 Main St|    Anytown|   KA|   12345|91-88929-88888|2025-12-28 17:38:...|20220101|\n",
      "|    S002|Pet House MH|    456 Elm St|Anothertown|   MH|   67890|91-99999-99999|2025-12-28 17:38:...|20220101|\n",
      "|    S003|Pet House TN|   789 Oak Ave|    Bigcity|   TN|    9876|91-77777-77777|2025-12-28 17:38:...|20220101|\n",
      "|    S004|Pet House OR|321 Birch Blvd| Small Town|   OR|   76684|91-88822-00000|2025-12-28 17:38:...|20220101|\n",
      "|    S005|Pet House WB|   654 Pine St|   Busytown|   WB|   11111|91-00002-22222|2025-12-28 17:38:...|20220101|\n",
      "|    S006|Pet House JK|  987 Cedar Rd|  Hill Town|   JK|   22222|91-33330-33333|2025-12-28 17:38:...|20220101|\n",
      "|    S007|Pet House GJ|  246 Maple St|   Anywhere|   GJ|   33333|91-55555-61000|2025-12-28 17:38:...|20220101|\n",
      "+--------+------------+--------------+-----------+-----+--------+--------------+--------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from edw_ld.dim_store_ld\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f706e385-a993-446a-8bee-6ca750b442bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
