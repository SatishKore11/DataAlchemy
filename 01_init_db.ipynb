{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27c4868a-f092-4c1c-af72-37d06738d89b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Script to initialize the Data Warehouse\\Lakehouse and create the required tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70e11750-ab43-4ef8-bf5d-1306d0f7eaeb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from lib.spark_session import get_spark_session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99d06d98-a4da-4a2a-bdd0-3eb54cfeca6d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPARK_APP: Spark Session UI - http://16804892cba9:4041\n"
     ]
    }
   ],
   "source": [
    "# Generate SparkSession\n",
    "spark: SparkSession = get_spark_session(\"Drop Databases\")\n",
    "print(\"SPARK_APP: Spark Session UI - \"+ spark.sparkContext.uiWebUrl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13b1197d-c1df-47f1-a55b-1d3cde38c14a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|  default|\n",
      "|      edw|\n",
      "|   edw_ld|\n",
      "|  edw_stg|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create dw schema in catalog\n",
    "spark.sql(\"create database if not exists edw\");\n",
    "spark.sql(\"create database if not exists edw_stg\");\n",
    "spark.sql(\"create database if not exists edw_ld\");\n",
    "spark.sql(\"show databases\").show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40e97c17-1ef9-4dac-a3da-dfb8ba2ce009",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPARK-APP: Store dimension created\n"
     ]
    }
   ],
   "source": [
    "# Create Store Dim table\n",
    "spark.sql(\"\"\"drop table if exists edw.dim_store\"\"\");\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "create table edw.dim_store (\n",
    "    row_wid string,\n",
    "    store_id string,\n",
    "    address string,\n",
    "    city string,\n",
    "    state string,\n",
    "    zip_code string,\n",
    "    phone_number string,\n",
    "    rundate string,\n",
    "    insert_dt timestamp,\n",
    "    update_dt timestamp\n",
    ")\n",
    "USING delta\n",
    ";\n",
    "\"\"\");\n",
    "\n",
    "print(\"SPARK-APP: Store dimension created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3d440c2-0056-4642-b893-68deab7a2b65",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPARK-APP: Plan Type dimension created\n"
     ]
    }
   ],
   "source": [
    "# Create Plan Type Dimension\n",
    "spark.sql(\"\"\"drop table if exists edw.dim_plan_type\"\"\");\n",
    "spark.sql(\"\"\"\n",
    "create table edw.dim_plan_type (\n",
    "    plan_type_code string,\n",
    "    plan_name string,\n",
    "    rundate string,\n",
    "    insert_dt timestamp,\n",
    "    update_dt timestamp\n",
    ")\n",
    "USING delta\n",
    ";\n",
    "\"\"\");\n",
    "print(\"SPARK-APP: Plan Type dimension created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05c92384-5d84-4aa8-9abd-3aca1b44ad19",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Table edw.dim_date already exists",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2255/4093076391.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mUSING\u001b[0m \u001b[0mdelta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \"\"\");\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"SPARK-APP: Date dimension created\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/spark/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery, **kwargs)\u001b[0m\n\u001b[1;32m   1032\u001b[0m             \u001b[0msqlQuery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1034\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1035\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1322\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    194\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Table edw.dim_date already exists"
     ]
    }
   ],
   "source": [
    "# Create Date Dimension\n",
    "spark.sql(\"\"\"drop table if exists edw.dim_table\"\"\");\n",
    "spark.sql(\"\"\"\n",
    "create table edw.dim_date (\n",
    "    row_wid string,\n",
    "    date date,\n",
    "    day int,\n",
    "    month int,\n",
    "    year int,\n",
    "    day_of_week string,\n",
    "    rundate string,\n",
    "    insert_dt timestamp,\n",
    "    update_dt timestamp\n",
    ")\n",
    "USING delta\n",
    ";\n",
    "\"\"\");\n",
    "\n",
    "print(\"SPARK-APP: Date dimension created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948602ac-c52d-4c22-a318-de627f7de004",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create Product Dimension\n",
    "spark.sql(\"\"\"drop table if exists edw.dim_product\"\"\");\n",
    "spark.sql(\"\"\"\n",
    "create table edw.dim_product (\n",
    "    row_wid string,\n",
    "    product_id string,\n",
    "    product_name string,\n",
    "    brand string,\n",
    "    type string,\n",
    "    flavor string,\n",
    "    size string,\n",
    "    price double,\n",
    "    expiration_dt date,\n",
    "    image_url string,\n",
    "    effective_start_dt timestamp,\n",
    "    effective_end_dt timestamp,\n",
    "    active_flag int,\n",
    "    rundate string,\n",
    "    insert_dt timestamp,\n",
    "    update_dt timestamp\n",
    ")\n",
    "USING delta\n",
    ";\n",
    "\"\"\");\n",
    "\n",
    "print(\"SPARK-APP: Product dimension created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b9d160-e073-49f3-900f-1267c85fc450",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create Customer Dimension\n",
    "spark.sql(\"\"\"drop table if exists edw.dim_customer\"\"\");\n",
    "spark.sql(\"\"\"\n",
    "create table edw.dim_customer (\n",
    "    row_wid string,\n",
    "    customer_id string,\n",
    "    first_name string,\n",
    "    last_name string,\n",
    "    address string,\n",
    "    city string,\n",
    "    state string,\n",
    "    zip_code string,\n",
    "    phone_number string,\n",
    "    email string,\n",
    "    date_of_birth date,\n",
    "    plan_type string,\n",
    "    effective_start_dt timestamp,\n",
    "    effective_end_date timestamp,\n",
    "    active_flg int,\n",
    "    rundate string,\n",
    "    insert_dt timestamp,\n",
    "    update_dt timestamp\n",
    ")\n",
    "USING delta\n",
    ";\n",
    "\"\"\");\n",
    "\n",
    "print(\"SPARK-APP: Customer dimension created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebad1f70-182d-403c-982e-30b87adc9480",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create Sales Fact\n",
    "spark.sql(\"\"\"drop table if exists edw.fact_sales\"\"\");\n",
    "spark.sql(\"\"\"\n",
    "create table edw.fact_sales (\n",
    "    date_wid string,\n",
    "    product_wid string,\n",
    "    store_wid string,\n",
    "    customer_wid string,\n",
    "    order_id string,\n",
    "    invoice_num string,\n",
    "    qty int,\n",
    "    tax double,\n",
    "    discount double,\n",
    "    line_total double,\n",
    "    integration_key string,\n",
    "    rundate string,\n",
    "    insert_dt timestamp,\n",
    "    update_dt timestamp\n",
    ")\n",
    "USING delta\n",
    ";\n",
    "\"\"\");\n",
    "\n",
    "print(\"SPARK-APP: Sales Fact created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ee8dec-70a4-4af9-9407-58534de3effd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create Audit table\n",
    "spark.sql(\"\"\"drop table if exists edw.job_control\"\"\");\n",
    "spark.sql(\"\"\"\n",
    "create table edw.job_control (\n",
    "    schema_name string,\n",
    "    table_name string,\n",
    "    max_timestamp timestamp,\n",
    "    rundate string,\n",
    "    insert_dt timestamp\n",
    ")\n",
    "USING delta\n",
    ";\n",
    "\"\"\");\n",
    "\n",
    "print(\"SPARK-APP: JOB Control table created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83bc159c-a465-4183-a9dc-cb36f1c9c6c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Log all tables in Data Warehouse/Lakehouse\n",
    "\n",
    "spark.sql(\"show tables in edw\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3bf13b8-4659-4de5-9efd-feef5a171b53",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbeb326c-918a-412c-b331-d9c0f3406812",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
